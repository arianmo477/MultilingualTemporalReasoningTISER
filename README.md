## TISER - Multilingual Extension

### Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models

This repository contains the **multilingual extension** for the TISER dataset from the paper (ACL 2025 Main): [Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models](https://arxiv.org/pdf/2504.05258).

TISER incorporates a multi-stage inference pipeline that combines explicit reasoning, timeline construction, and iterative self-reflection. The key idea behind our approach is to empower LLMs to adapt by scaling their internal reasoning process during inference. TISER enables models to systematically organize temporal information, verify their inferences, and refine their outputs.

### ğŸŒ Multilingual Extension

This repository extends TISER to **multiple languages**:
- ğŸ‡®ğŸ‡¹ **Italian (it)**
- ğŸ‡®ğŸ‡· **Persian/Farsi (fa)**
- ğŸ‡©ğŸ‡ª **German (de)**

**Key Features:**
- âœ… High-quality neural machine translation with entity preservation
- âœ… Language-specific temporal expression normalization
- âœ… Chain-of-thought structure preservation across languages
- âœ… Comprehensive validation and quality control
- âœ… Ready-to-use scripts for the complete translation pipeline

ğŸ“– **See [USAGE.md](USAGE.md) for detailed instructions!**

## Train Data Format

Each entry in the [TISER train dataset](data/TISER_train.json) is a JSON object containing six fields: `dataset_name`, `question_id`, `question`, `answer`, `prompt`, and `output`. The `question` field specifies the temporal question being asked, while `answer` contains the expected short-form response (e.g., an entity or number). The `prompt` provides detailed instructions for a Chain of Thought (CoT) reasoning process with reflection, guiding the model to reason step-by-step, extract temporal events, reflect on its logic, and produce a final answer. The `output` field contains the full model-generated response adhering to this reasoning format. This structure supports supervised training of models to perform temporal reasoning and answer generation.

## Test Data Format

Each test example in the [TISER test dataset](data/TISER_test.json) is represented as a single JSON object containing five fields: `dataset_name`, which specifies the split or task; `question_id`, a unique identifier for the query; `question`, the temporal reasoning prompt itself; `prompt`, which embeds the full Chain-of-Thought template (including `<reasoning>`, `<timeline>`, `<reflection>` and `<answer>` tags) that the model should follow when generating its response; and `answer`, the held-out ground-truth output against which the modelâ€™s `<answer>` section is evaluated. Unlike the training format, there is no `output` field in test records, since models read the `prompt` and produce only an `<answer>` that is scored directly against the provided `answer` key.

## Data Subsets Provenance

Original data subsets before our preprocessing were extracted from the following HuggingFace URLs

- TGQA: [https://huggingface.co/datasets/sxiong/TGQA/viewer/TGQA_TGR](https://huggingface.co/datasets/sxiong/TGQA/viewer/TGQA_TGR)
- TempReason (L2): [https://huggingface.co/datasets/sxiong/TGQA/viewer/TempReason_TGR/l2_train](https://huggingface.co/datasets/sxiong/TGQA/viewer/TempReason_TGR/l2_train)
- TempReason (L3): [https://huggingface.co/datasets/sxiong/TGQA/viewer/TempReason_TGR/l3_train](https://huggingface.co/datasets/sxiong/TGQA/viewer/TempReason_TGR/l3_train)
- TimeQA (easy): [https://huggingface.co/datasets/sxiong/TGQA/viewer/TimeQA_TGR/easy_train](https://huggingface.co/datasets/sxiong/TGQA/viewer/TimeQA_TGR/easy_train)
- TimeQA (hard): [https://huggingface.co/datasets/sxiong/TGQA/viewer/TimeQA_TGR/hard_train](https://huggingface.co/datasets/sxiong/TGQA/viewer/TimeQA_TGR/hard_train)

## Citation
```
@misc{bazaga2025learningreasontimetimeline,
      title={Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models}, 
      author={AdriÃ¡n Bazaga and Rexhina Blloshmi and Bill Byrne and AdriÃ  de Gispert},
      year={2025},
      eprint={2504.05258},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2504.05258}, 
}
```

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This library is licensed under the MIT-0 License. See the [LICENSE](LICENSE)  file.

## Quick Start

### Installation
```bash
# Create environment
conda env create -f environment.yml
conda activate tiser

# Make scripts executable
chmod +x scripts/*.sh
```

### Translate to a Single Language
```bash
# Translate 1000 samples to Italian (for testing)
python multilingual_tiser/translation/translate_dataset.py \
    --input data/TISER_train.json \
    --output_dir data/translated \
    --category train \
    --language it \
    --batch_size 8 \
    --max_samples 1000
```

### Run Full Pipeline
```bash
# Translate all languages, validate, and generate statistics
./scripts/run_full_pipeline.sh
```

### Example Scripts
```bash
./scripts/example_translate_single_language.sh   # Quick translation
./scripts/example_validate_and_clean.sh          # Quality control
./scripts/example_analyze_statistics.sh          # Statistics
```

## Repository Structure

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ TISER_train.json              # Original English training data
â”‚   â”œâ”€â”€ TISER_test.json               # Original English test data
â”‚   â””â”€â”€ prompts/                      # Multilingual prompts
â”‚       â”œâ”€â”€ prompt_en.txt
â”‚       â”œâ”€â”€ prompt_it.txt
â”‚       â”œâ”€â”€ prompt_fa.txt
â”‚       â””â”€â”€ prompt_de.txt
â”œâ”€â”€ multilingual_tiser/
â”‚   â”œâ”€â”€ translation/
â”‚   â”‚   â””â”€â”€ translate_dataset.py      # Translation pipeline
â”‚   â””â”€â”€ preprocess/
â”‚       â”œâ”€â”€ validate_tiser_dataset.py # Data validation & cleaning
â”‚       â””â”€â”€ build_mixed_dataset.py    # Create multilingual datasets
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ evaluation.py                 # Multilingual evaluation metrics
â”‚   â”œâ”€â”€ text_processing.py            # Language-specific processing
â”‚   â””â”€â”€ io_gpu.py                     # GPU utilities
â””â”€â”€ scripts/
    â”œâ”€â”€ run_full_pipeline.sh          # Complete automation
    â”œâ”€â”€ validate_translation_quality.py
    â”œâ”€â”€ analyze_dataset_statistics.py
    â””â”€â”€ example_*.sh                  # Usage examples
```

## Documentation

- **[USAGE.md](USAGE.md)** - Comprehensive usage guide
- **[CONTRIBUTING.md](CONTRIBUTING.md)** - Contribution guidelines
- **[CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md)** - Code of conduct

## Contact

For feedback or questions about the multilingual extension, please open an issue.

For questions about the original TISER paper, please contact [AdriÃ¡n Bazaga](https://bazaga.ai/)